{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file will follow tutorial \n",
    "https://www.depends-on-the-definition.com/named-entity-recognition-conditional-random-fields-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_all_line( directory , file ):\n",
    "    file = open( directory + \"/\" + file , \"r\" , encoding=\"utf-8\" )\n",
    "    contents = file.read()\n",
    "    file.close()\n",
    "    return contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_directory = \"dictionary_directory\"\n",
    "dictionary_files = {\n",
    "    \"front_person\" : (\"clue front.txt\" , \"clue word person.txt\" , \"clue_royal.txt\")\n",
    "    ,\"front_country\" : (\"คำนำหน้าชื่อประเทศ.txt\")\n",
    "    ,\"front_org\" : (\"คำนำหน้าองค์กรจาก dict.txt\")\n",
    "    ,\"location_name\" : (\"ชื่อกิ่งอำเภอ.txt\" , \"ชื่อคลอง.txt\" , \"ชื่อจังหวัด.txt\" , \"ชื่อตำบล.txt\" , \"ชื่อมลรัฐ.txt\" , \"ชื่อสถานที่.txt\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dictionary( directory , file_dict ):\n",
    "    dictionary = {}\n",
    "    for key in file_dict.keys():\n",
    "        dictionary[ key ] = []\n",
    "        list_file = file_dict[ key ]\n",
    "        if type( list_file ) == type( \"test_string\" ):\n",
    "            dictionary[ key ] = read_all_line( directory , list_file ).split('\\n')\n",
    "        else:\n",
    "            for file in list_file:\n",
    "                dictionary[ key ] += read_all_line( directory , file ).split('\\n')\n",
    "    return dictionary\n",
    "\n",
    "# If found in dict will return key other return NO\n",
    "def search_in_dictionary( word , dictionary ):\n",
    "    answer = \"NO\"\n",
    "    for key in dictionary.keys():\n",
    "        if word in dictionary[key]:\n",
    "            answer = key\n",
    "            break\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = prepare_dictionary( dictionary_directory , dictionary_files )\n",
    "search_in_dictionary( \"และ\" , dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idividual_read_file( raw_data , split_sentence , count ,remove = [] ):\n",
    "    pre_data_frame = { \"Sentence #\" : [] , \"Word\" : [] , \"Tag\" : [] , \"Dictionary\" : [] }\n",
    "    for word in raw_data:\n",
    "        if word in remove :\n",
    "            None\n",
    "        elif word in split_sentence :\n",
    "            count += 1\n",
    "        else: \n",
    "            check = word.find('(')\n",
    "            if check > 0:\n",
    "                temp_word = word[ 0 : word.find('(')]\n",
    "                pre_data_frame[\"Sentence #\"].append( \"Sentence: \" + str(count) )\n",
    "                pre_data_frame[\"Word\"].append( temp_word )\n",
    "                pre_data_frame[\"Tag\"].append( word[ word.find('(') + 1 : word.find(')')] )\n",
    "                pre_data_frame[\"Dictionary\"].append( search_in_dictionary( temp_word , dictionary ) )\n",
    "            else:\n",
    "                pre_data_frame[\"Sentence #\"].append( \"Sentence: \" + str(count) )\n",
    "                pre_data_frame[\"Word\"].append( word )\n",
    "                pre_data_frame[\"Tag\"].append( \"O\" )\n",
    "                pre_data_frame[\"Dictionary\"].append( search_in_dictionary( word , dictionary ) )\n",
    "    return pre_data_frame , count\n",
    "\n",
    "def read_file( directory , list_file , count = 1 , individual = False ):\n",
    "    data_frame = pd.DataFrame( { \"Sentence #\" : [] , \"Word\" : [] , \"Tag\" : [] , \"Dictionary\" : []} )\n",
    "    if( individual ):\n",
    "        word = read_all_line( directory , list_file ).split('|')\n",
    "        pre_data_frame , count = idividual_read_file( word , # raw_data\n",
    "                                              [\"\\n\"] , # word show split sentence\n",
    "                                              count , # order of sentence\n",
    "                                              [ \" \" , '' , '\\0'] ) # word to delete or prevent\n",
    "        data_frame  = data_frame.append( pd.DataFrame( pre_data_frame ), ignore_index=True )\n",
    "    else:\n",
    "        for file in list_file :\n",
    "            word = read_all_line( directory , file ).split('|')\n",
    "            pre_data_frame , count = idividual_read_file( word ,\n",
    "                                              [\"\\n\"] ,\n",
    "                                              count ,\n",
    "                                              [ \" \" , '' , '\\0'] )\n",
    "            data_frame  = data_frame.append( pd.DataFrame( pre_data_frame ), ignore_index=True )\n",
    "    return data_frame\n",
    "def num_of_sentence( sentence ):\n",
    "    words = sentence.split( \" \")\n",
    "    return int( words[1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable for collect name file and access file\n",
    "train_directory = \"corpus_directory\"\n",
    "train_files = (\"Politic9.txt\" , \"Allcolumn.txt\" , \"98JUL5_1.txt\" ,\n",
    "                \"98JUL5_2.txt\" , \"bkknews1.txt\" , \"Result1.txt\" ,\n",
    "                \"Result2(corpus-1).txt\" , \"Result3(corpus-2).txt\" )\n",
    "test_directory = \"corpus_directory\"\n",
    "test_files = (\"result4.txt\" , \"Result_c_2_1.txt\", \"Result_c_2_2.txt\" )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ข้างบนทั้งหมด คือการที่เราจะอ่าน text data แล้วได้ข้อมูลออกมาเป็น pandas.DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_file( train_directory , train_files , 1 )\n",
    "data.fillna( method=\"ffill\")\n",
    "data.tail( 10 )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "เนื่องจาก tag หลากหลายมาก ส่วนเริ่มต้น ส่วนต่อ และส่วนจบ จึงจะจัดการให้เหลือเพียงส่วนเดียว"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_all_tag( data ):\n",
    "    for run in range(  0 , data.shape[0] ):\n",
    "        if data.Tag[run] in ( '' ):\n",
    "            data.Tag[run] = 'O'\n",
    "        elif data.Tag[run] in ('loc_cont','loc_end','loc_start'):\n",
    "            data.Tag[run] = 'loc'\n",
    "        elif data.Tag[run] in ('org_cont','org_end','org_start'):\n",
    "            data.Tag[run] = 'org'\n",
    "        elif data.Tag[run] in ('per_cont','per_end','per_start'):\n",
    "            data.Tag[run] = 'per'\n",
    "        else:\n",
    "            None\n",
    "\n",
    "# number is current index\n",
    "# target is -1 -2 +1 +2 what do you want to file\n",
    "def helper_get_tag( data , number , target , limit ):\n",
    "    if( number + target == limit ):\n",
    "        return 'O'\n",
    "    elif( data[\"Sentence #\"][ number+target ] != data[\"Sentence #\"][ number ] ):\n",
    "        return 'O'\n",
    "    else:\n",
    "        return data.Tag[number + target ]\n",
    "            \n",
    "def split_all_tag( data ):\n",
    "    limit_run = data.shape[0]\n",
    "    for run in range( 0 , limit_run ):\n",
    "        if data.Tag[ run ] == \"loc\" :\n",
    "            if( helper_get_tag( data , run , -1, limit_run  ) in ( \"loc_start\" , \"loc_cont\" ) ):\n",
    "                if( helper_get_tag( data , run , +1 , limit_run ) == \"loc\" ):\n",
    "                    data.Tag[ run ] = \"loc_cont\"\n",
    "                else:\n",
    "                    data.Tag[ run ] = \"loc_end\"\n",
    "            elif( helper_get_tag( data , run , +1 , limit_run ) == \"loc\" ):\n",
    "                data.Tag[ run ] = \"loc_start\"\n",
    "        elif data.Tag[ run ] == \"org\" :\n",
    "            if( helper_get_tag( data , run , -1 , limit_run ) in ( \"org_start\" , \"org_cont\" ) ):\n",
    "                if( helper_get_tag( data , run , +1 , limit_run ) == \"org\" ):\n",
    "                    data.Tag[ run ] = \"org_cont\"\n",
    "                else:\n",
    "                    data.Tag[ run ] = \"org_end\"\n",
    "            elif( helper_get_tag( data , run , +1 , limit_run ) == \"org\" ):\n",
    "                data.Tag[ run ] = \"org_start\"\n",
    "        elif data.Tag[ run ] == \"per\" :\n",
    "            if( helper_get_tag( data , run , -1 , limit_run ) in ( \"per_start\" , \"per_cont\" ) ):\n",
    "                if( helper_get_tag( data , run , +1, limit_run ) == \"per\" ):\n",
    "                    data.Tag[ run ] = \"per_cont\"\n",
    "                else:\n",
    "                    data.Tag[ run ] = \"per_end\"\n",
    "            elif( helper_get_tag( data , run , +1 , limit_run ) == \"per\" ):\n",
    "                data.Tag[ run ] = \"per_start\"\n",
    "            else:\n",
    "                None\n",
    "        else:\n",
    "            None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_all_tag( data )\n",
    "words = list( set(data[\"Word\"].values ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_words = len( words )\n",
    "n_sentence = num_of_sentence( data[\"Sentence #\"][ data.shape[0]-1 ] )\n",
    "print( \"You have number of words is {} from {} sentences\".format( n_words , n_sentence ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceGetter(object):\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.n_sent = 0\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, t, d) for w, t, d in zip( s[\"Word\"].values.tolist(),\n",
    "                                                            s[\"Tag\"].values.tolist(),\n",
    "                                                            s[\"Dictionary\"].values.tolist() \n",
    "                                                          )]\n",
    "        self.grouped = self.grouped_sentence()\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "    \n",
    "    def grouped_sentence( self ):\n",
    "        save_sentence = 0\n",
    "        major_index = 0\n",
    "        temp_grouped = []\n",
    "        for run in range( 0 , self.data.shape[0] ):\n",
    "            temp_sentence = num_of_sentence( self.data[\"Sentence #\"][ run ] )\n",
    "            if save_sentence != temp_sentence :\n",
    "                major_index = temp_sentence - 1\n",
    "                save_sentence = temp_sentence\n",
    "                temp_grouped.append( [] )\n",
    "            temp_grouped[ major_index ].append( (self.data[\"Word\"][run], \n",
    "                                                 self.data[\"Tag\"][run],\n",
    "                                                 self.data[\"Dictionary\"][run] ) )\n",
    "        return temp_grouped\n",
    "    \n",
    "    def get_next(self):\n",
    "        try:\n",
    "            s = self.grouped[ self.n_sent ]\n",
    "            self.n_sent += 1\n",
    "            return s  \n",
    "        except:\n",
    "            self.empty = True\n",
    "            return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getter = SentenceGetter( data )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent= getter.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ข้างบนนั้นจะดึงออกมาทีละประโยค โดยที่จะดึงคำออกมาพร้อมกับ tag นั้นเอง\n",
    "ดังนั้นต่อมาเราจะดึงออกทั้งหมด ทุกประโยคเลย"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = getter.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    dict_type = sent[i][2]\n",
    "\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        'dictionary' : dict_type\n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        dict_type1 = sent[i-1][2]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "            '-1:dictionay' : dict_type1\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "\n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        dict_type1 = sent[i+1][2]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "            '-1:dictionay' : dict_type1\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, label, dict_type in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, label, dict_type in sent]\n",
    "\n",
    "def sent2dict_type( sent ):\n",
    "    return [dict_type for token, label, dict_type in sent]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ข้างบนเป็นการจัดรูปแบบ feature ของคำเฉยๆ ถ้าอยากรู้จะต้องอ่านเนื้อหาเกี่ยวกับ CRF เพิ่มเติมว่ามีแนวคิดอย่างไร คาดว่าจุดนี้อาจทำให้สามารถคำนวณโดยเอา Dict มาใช้ได้\n",
    "https://medium.com/ml2vec/overview-of-conditional-random-fields-68a2a20fa541"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn_crfsuite import CRF\n",
    "\n",
    "crf = CRF(algorithm='lbfgs',\n",
    "          c1=0.1,\n",
    "          c2=0.1,\n",
    "          max_iterations=100,\n",
    "          all_possible_transitions=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref for first module https://scikit-learn.org/stable/modules/cross_validation.html\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn_crfsuite.metrics import flat_classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [sent2features(s) for s in sentences]\n",
    "y = [sent2labels(s) for s in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = cross_val_predict(estimator=crf, X=X, y=y, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = flat_classification_report(y_pred=pred, y_true=y)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import eli5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eli5.show_weights(crf, top=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference https://machinelearningmastery.com/save-load-machine-learning-models-python-scikit-learn/\n",
    "import pickle\n",
    "model_directory = \"model_directory\"\n",
    "model_name = \"CRF_dictionary.sav\"\n",
    "model_file = model_directory + \"/\" + model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( crf , open( model_file , 'wb') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = pickle.load( open( model_file , 'rb') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = read_file( test_directory , test_files , 1 )\n",
    "data_test.fillna( method=\"ffill\")\n",
    "data_test.tail( 10 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_all_tag( data_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getter_test = SentenceGetter( data_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getter_test.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_test = getter_test.sentences\n",
    "sentences_test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = [sent2features(s) for s in sentences_test]\n",
    "test_label = [sent2labels(s) for s in sentences_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict = load_model.predict( test_set )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = flat_classification_report(y_pred=test_predict, y_true=test_label)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.head( 20 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_list_to_data_frame( data_frame , data_list ):\n",
    "    save_sentence = 0\n",
    "    major_index_list = 0\n",
    "    minor_index_list = 0\n",
    "    for run in range( 0 , data_frame.shape[0] ):\n",
    "        temp_sentence = num_of_sentence( data_frame[\"Sentence #\"][ run ] )\n",
    "        if save_sentence != temp_sentence :\n",
    "            major_index_list = temp_sentence - 1\n",
    "            save_sentence = temp_sentence\n",
    "            minor_index_list = 0\n",
    "        print( \"access [{}][{}]\".format( major_index_list , minor_index_list ))\n",
    "        data_frame[\"Tag\"][run] = data_list[ major_index_list][ minor_index_list ]\n",
    "        minor_index_list += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_list_to_data_frame( data_test , test_predict )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_all_tag( data_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
