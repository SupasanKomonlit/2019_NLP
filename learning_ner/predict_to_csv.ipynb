{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn_crfsuite import CRF\n",
    "# ref for first module https://scikit-learn.org/stable/modules/cross_validation.html\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn_crfsuite.metrics import flat_classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_all_line( directory , file ):\n",
    "    file = open( directory + \"/\" + file , \"r\" , encoding=\"utf-8\" )\n",
    "    contents = file.read()\n",
    "    file.close()\n",
    "    return contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    dict_type = sent[i][2]\n",
    "\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        'dictionary' : dict_type\n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        dict_type1 = sent[i-1][2]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "            '-1:dictionay' : dict_type1\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "\n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        dict_type1 = sent[i+1][2]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "            '-1:dictionay' : dict_type1\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, label, dict_type in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, label, dict_type in sent]\n",
    "\n",
    "def sent2dict_type( sent ):\n",
    "    return [dict_type for token, label, dict_type in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceGetter(object):\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.n_sent = 0\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, t, d) for w, t, d in zip( s[\"Word\"].values.tolist(),\n",
    "                                                            s[\"Tag\"].values.tolist(),\n",
    "                                                            s[\"Dictionary\"].values.tolist() \n",
    "                                                          )]\n",
    "        self.grouped = self.grouped_sentence()\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "    \n",
    "    def grouped_sentence( self ):\n",
    "        save_sentence = 0\n",
    "        major_index = 0\n",
    "        temp_grouped = []\n",
    "        for run in range( 0 , self.data.shape[0] ):\n",
    "            temp_sentence = num_of_sentence( self.data[\"Sentence #\"][ run ] )\n",
    "            if save_sentence != temp_sentence :\n",
    "                major_index = temp_sentence - 1\n",
    "                save_sentence = temp_sentence\n",
    "                temp_grouped.append( [] )\n",
    "            temp_grouped[ major_index ].append( (self.data[\"Word\"][run], \n",
    "                                                 self.data[\"Tag\"][run],\n",
    "                                                 self.data[\"Dictionary\"][run] ) )\n",
    "        return temp_grouped\n",
    "    \n",
    "    def get_next(self):\n",
    "        try:\n",
    "            s = self.grouped[ self.n_sent ]\n",
    "            self.n_sent += 1\n",
    "            return s  \n",
    "        except:\n",
    "            self.empty = True\n",
    "            return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tag_group_sentence( data ):\n",
    "    old_tag = []\n",
    "    save_sentence = \"\"\n",
    "    run_old = -1\n",
    "    for run in range(  0 , data.shape[0] ):\n",
    "        if( save_sentence != data[\"Sentence #\"][ run ] ):\n",
    "            old_tag.append( [] )\n",
    "            run_old += 1\n",
    "        else:\n",
    "            None\n",
    "        if data.Tag[run] in ( '' ):\n",
    "            old_tag[ run_old ].append( 'O' )\n",
    "        else:\n",
    "            old_tag[ run_old ].append( data.Tag[ run ] )\n",
    "    return old_tag\n",
    "\n",
    "def merge_all_tag( data ):\n",
    "    old_tag = []\n",
    "    save_sentence = \"\"\n",
    "    run_old = -1\n",
    "    for run in range(  0 , data.shape[0] ):\n",
    "        if( save_sentence != data[\"Sentence #\"][ run ] ):\n",
    "            old_tag.append( [] )\n",
    "            run_old += 1\n",
    "        else:\n",
    "            None\n",
    "        if data.Tag[run] in ( '' ):\n",
    "            old_tag[ run_old ].append( 'O' )\n",
    "            data.Tag[run] = 'O'\n",
    "        elif data.Tag[run] in ('loc_cont','loc_end','loc_start'):\n",
    "            old_tag[ run_old ].append( data.Tag[ run ] )\n",
    "            data.Tag[run] = 'loc'\n",
    "        elif data.Tag[run] in ('org_cont','org_end','org_start'):\n",
    "            old_tag[ run_old ].append( data.Tag[ run ] )\n",
    "            data.Tag[run] = 'org'\n",
    "        elif data.Tag[run] in ('per_cont','per_end','per_start'):\n",
    "            old_tag[ run_old ].append( data.Tag[ run ] )\n",
    "            data.Tag[run] = 'per'\n",
    "        else:\n",
    "            old_tag[ run_old ].append( data.Tag[ run ] )\n",
    "            None\n",
    "    return old_tag\n",
    "\n",
    "# number is current index\n",
    "# target is -1 -2 +1 +2 what do you want to file\n",
    "def helper_get_tag( data , number , target , limit ):\n",
    "    if( number + target == limit ):\n",
    "        return 'O'\n",
    "    elif( data[\"Sentence #\"][ number+target ] != data[\"Sentence #\"][ number ] ):\n",
    "        return 'O'\n",
    "    else:\n",
    "        return data.Tag[number + target ]\n",
    "            \n",
    "def split_all_tag( data ):\n",
    "    limit_run = data.shape[0]\n",
    "    for run in range( 0 , limit_run ):\n",
    "        if data.Tag[ run ] == \"loc\" :\n",
    "            if( helper_get_tag( data , run , -1, limit_run  ) in ( \"loc_start\" , \"loc_cont\" ) ):\n",
    "                if( helper_get_tag( data , run , +1 , limit_run ) == \"loc\" ):\n",
    "                    data.Tag[ run ] = \"loc_cont\"\n",
    "                else:\n",
    "                    data.Tag[ run ] = \"loc_end\"\n",
    "            elif( helper_get_tag( data , run , +1 , limit_run ) == \"loc\" ):\n",
    "                data.Tag[ run ] = \"loc_start\"\n",
    "        elif data.Tag[ run ] == \"org\" :\n",
    "            if( helper_get_tag( data , run , -1 , limit_run ) in ( \"org_start\" , \"org_cont\" ) ):\n",
    "                if( helper_get_tag( data , run , +1 , limit_run ) == \"org\" ):\n",
    "                    data.Tag[ run ] = \"org_cont\"\n",
    "                else:\n",
    "                    data.Tag[ run ] = \"org_end\"\n",
    "            elif( helper_get_tag( data , run , +1 , limit_run ) == \"org\" ):\n",
    "                data.Tag[ run ] = \"org_start\"\n",
    "        elif data.Tag[ run ] == \"per\" :\n",
    "            if( helper_get_tag( data , run , -1 , limit_run ) in ( \"per_start\" , \"per_cont\" ) ):\n",
    "                if( helper_get_tag( data , run , +1, limit_run ) == \"per\" ):\n",
    "                    data.Tag[ run ] = \"per_cont\"\n",
    "                else:\n",
    "                    data.Tag[ run ] = \"per_end\"\n",
    "            elif( helper_get_tag( data , run , +1 , limit_run ) == \"per\" ):\n",
    "                data.Tag[ run ] = \"per_start\"\n",
    "            else:\n",
    "                None\n",
    "        else:\n",
    "            None\n",
    "\n",
    "def convert_list_to_data_frame( data_frame , data_list ):\n",
    "    save_sentence = 0\n",
    "    major_index_list = 0\n",
    "    minor_index_list = 0\n",
    "    for run in range( 0 , data_frame.shape[0] ):\n",
    "        temp_sentence = num_of_sentence( data_frame[\"Sentence #\"][ run ] )\n",
    "        if save_sentence != temp_sentence :\n",
    "            major_index_list = temp_sentence - 1\n",
    "            save_sentence = temp_sentence\n",
    "            minor_index_list = 0\n",
    "        data_frame[\"Tag\"][run] = data_list[ major_index_list][ minor_index_list ]\n",
    "        minor_index_list += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dictionary( directory , file_dict ):\n",
    "    dictionary = {}\n",
    "    for key in file_dict.keys():\n",
    "        dictionary[ key ] = []\n",
    "        list_file = file_dict[ key ]\n",
    "        if type( list_file ) == type( \"test_string\" ):\n",
    "            dictionary[ key ] = read_all_line( directory , list_file ).split('\\n')\n",
    "        else:\n",
    "            for file in list_file:\n",
    "                dictionary[ key ] += read_all_line( directory , file ).split('\\n')\n",
    "    return dictionary\n",
    "\n",
    "# If found in dict will return key other return NO\n",
    "def search_in_dictionary( word , dictionary ):\n",
    "    answer = \"NO\"\n",
    "    for key in dictionary.keys():\n",
    "        if word in dictionary[key]:\n",
    "            answer = key\n",
    "            break\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idividual_read_file( raw_data , split_sentence , count ,remove = [] ):\n",
    "    pre_data_frame = { \"Sentence #\" : [] , \"Word\" : [] , \"Tag\" : [] , \"Dictionary\" : [] }\n",
    "    for word in raw_data:\n",
    "        if word in remove :\n",
    "            None\n",
    "        elif word in split_sentence :\n",
    "            count += 1\n",
    "        else: \n",
    "            check = word.find('(')\n",
    "            if check > 0:\n",
    "                temp_word = word[ 0 : word.find('(')]\n",
    "                pre_data_frame[\"Sentence #\"].append( \"Sentence: \" + str(count) )\n",
    "                pre_data_frame[\"Word\"].append( temp_word )\n",
    "                pre_data_frame[\"Tag\"].append( word[ word.find('(') + 1 : word.find(')')] )\n",
    "                pre_data_frame[\"Dictionary\"].append( search_in_dictionary( temp_word , dictionary ) )\n",
    "            else:\n",
    "                pre_data_frame[\"Sentence #\"].append( \"Sentence: \" + str(count) )\n",
    "                pre_data_frame[\"Word\"].append( word )\n",
    "                pre_data_frame[\"Tag\"].append( \"O\" )\n",
    "                pre_data_frame[\"Dictionary\"].append( search_in_dictionary( word , dictionary ) )\n",
    "    return pre_data_frame , count\n",
    "\n",
    "def read_file( directory , list_file , count = 1 , individual = False ):\n",
    "    data_frame = pd.DataFrame( { \"Sentence #\" : [] , \"Word\" : [] , \"Tag\" : [] , \"Dictionary\" : []} )\n",
    "    if( individual ):\n",
    "        word = read_all_line( directory , list_file ).split('|')\n",
    "        pre_data_frame , count = idividual_read_file( word , # raw_data\n",
    "                                              [\"\\n\"] , # word show split sentence\n",
    "                                              count , # order of sentence\n",
    "                                              [ \" \" , '' , '\\0'] ) # word to delete or prevent\n",
    "        data_frame  = data_frame.append( pd.DataFrame( pre_data_frame ), ignore_index=True )\n",
    "    else:\n",
    "        for file in list_file :\n",
    "            word = read_all_line( directory , file ).split('|')\n",
    "            pre_data_frame , count = idividual_read_file( word ,\n",
    "                                              [\"\\n\"] ,\n",
    "                                              count ,\n",
    "                                              [ \" \" , '' , '\\0'] )\n",
    "            data_frame  = data_frame.append( pd.DataFrame( pre_data_frame ), ignore_index=True )\n",
    "    return data_frame\n",
    "def num_of_sentence( sentence ):\n",
    "    words = sentence.split( \" \")\n",
    "    return int( words[1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_directory = \"dictionary_directory\"\n",
    "dictionary_files = {\n",
    "    \"front_person\" : (\"clue front.txt\" , \"clue word person.txt\" , \"clue_royal.txt\")\n",
    "    ,\"front_country\" : (\"คำนำหน้าชื่อประเทศ.txt\")\n",
    "    ,\"front_org\" : (\"คำนำหน้าองค์กรจาก dict.txt\")\n",
    "    ,\"location_name\" : (\"ชื่อกิ่งอำเภอ.txt\" , \"ชื่อคลอง.txt\" , \"ชื่อจังหวัด.txt\" , \"ชื่อตำบล.txt\" , \"ชื่อมลรัฐ.txt\" , \"ชื่อสถานที่.txt\")\n",
    "}\n",
    "dictionary = prepare_dictionary( dictionary_directory , dictionary_files )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_directory = \"TestnoTag\"\n",
    "test_files = ( \"POL1101.CUT\" , \"POL1102.CUT\" , \"POL1103.CUT\" , \n",
    "               \"POL1104.CUT\" , \"POL1106.CUT\" , \n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_directory = \"model_directory\"\n",
    "model_name = \"CRF_dictionary.sav\"\n",
    "model_file = model_directory + \"/\" + model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = pickle.load( open( model_file , 'rb') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = read_file( test_directory , test_files , 1 )\n",
    "data_test.fillna( method=\"ffill\")\n",
    "old_tag_test = merge_all_tag( data_test )\n",
    "getter_test = SentenceGetter( data_test )\n",
    "sentences_test = getter_test.sentences\n",
    "test_set = [sent2features(s) for s in sentences_test]\n",
    "test_predict = load_model.predict( test_set )\n",
    "convert_list_to_data_frame( data_test , test_predict )\n",
    "split_all_tag( data_test )\n",
    "test_predict = get_tag_group_sentence( data_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.to_csv( \n",
    "        path_or_buf = test_directory + \"/result.csv\" ,\n",
    "        index=True );"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
