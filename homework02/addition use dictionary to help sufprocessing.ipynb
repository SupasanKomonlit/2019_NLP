{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file will learn follow reference https://towardsdatascience.com/named-entity-recognition-ner-meeting-industrys-requirement-by-applying-state-of-the-art-deep-698d2b3b4ede"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_all_line( directory , file ):\n",
    "    file = open( directory + \"/\" + file , \"r\" , encoding=\"utf-8\" )\n",
    "    contents = file.read()\n",
    "    return contents"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "I at this part to read information about dictionay to hope it will help you about data befor train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_directory = \"dictionary_directory\"\n",
    "dictionary = {\n",
    "    \"per_front\" : read_all_line( dictionary_directory , \"clue front.txt\").split('\\n') +\n",
    "            read_all_line( dictionary_directory , \"person title.txt\").split('\\n') ,\n",
    "    \"org_front\" : read_all_line( dictionary_directory , \"คำนำหน้าองค์กรจาก dict.txt\").split('\\n'),\n",
    "    \"loc\" : read_all_line( dictionary_directory , \"ชื่อคลอง.txt\").split('\\n') +\n",
    "            read_all_line( dictionary_directory , \"ชื่อมลรัฐ.txt\").split('\\n') +\n",
    "            read_all_line( dictionary_directory , \"ชื่อสถานที่.txt\").split('\\n') +\n",
    "            read_all_line( dictionary_directory , \"ชื่อจังหวัด.txt\").split('\\n') +\n",
    "            read_all_line( dictionary_directory , \"ชื่อกิ่งอำเภอ.txt\").split('\\n') +\n",
    "            read_all_line( dictionary_directory , \"ชื่ออำเภอ.txt\").split('\\n'),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dictionary( word ):\n",
    "    result_have = False\n",
    "    result_tag = \"O\"\n",
    "    for key in dictionary.keys():\n",
    "        if( word in dictionary[ key ] ):\n",
    "            result_have = True\n",
    "            result_tag = key\n",
    "            break\n",
    "    return result_have , result_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suffix_processing( word_sequence , result_predict ):\n",
    "    result = result_predict\n",
    "    for run in range( len(word_sequence) ):\n",
    "        have , tag = check_dictionay( word_sequence[ run ] )\n",
    "        if( have ):\n",
    "            result[ run ] = tag\n",
    "            if( tag == \"pre_front\" ):\n",
    "                if result[ run + 1 ] == \"O\":\n",
    "                    result[ run + 1 ] = \"per\"\n",
    "            elif( tag == \"org_front\" ):\n",
    "                None\n",
    "            else:\n",
    "                None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def individual_preprocess( raw_data , split_sentence , count ,remove = [] , use_dictionary = False ):\n",
    "    pre_data_frame = { \"Sentence\" : [] , \"Word\" : [] , \"Tag\" : [] }\n",
    "    for word in raw_data:\n",
    "        have = False\n",
    "        if use_dictionary:\n",
    "            have , tag = check_dictionary( word )\n",
    "            \n",
    "        if word in remove :\n",
    "            None\n",
    "        elif have :\n",
    "            pre_data_frame[\"Sentence\"].append( \"Sentence: \" + str(count) )\n",
    "            pre_data_frame[\"Word\"].append( word )\n",
    "            pre_data_frame[\"Tag\"].append( tag )\n",
    "        elif word in split_sentence :\n",
    "            count += 1\n",
    "        else: \n",
    "            check = word.find('(')\n",
    "            if check > 0:\n",
    "                pre_data_frame[\"Sentence\"].append( \"Sentence: \" + str(count) )\n",
    "                pre_data_frame[\"Word\"].append( word[ 0 : word.find('(')] )\n",
    "                pre_data_frame[\"Tag\"].append( word[ word.find('(') + 1 : word.find(')')] )\n",
    "            else:\n",
    "                pre_data_frame[\"Sentence\"].append( \"Sentence: \" + str(count) )\n",
    "                pre_data_frame[\"Word\"].append( word )\n",
    "                pre_data_frame[\"Tag\"].append( \"O\" )\n",
    "    return pre_data_frame , count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def preprocess( directory , list_file , count = 1 , individual = False , use_dictionary = False ):\n",
    "    data_frame = pd.DataFrame( { \"Sentence\" : [] , \"Word\" : [] , \"Tag\" : [] } )\n",
    "    if( individual ):\n",
    "        word = read_all_line( directory , list_file ).split('|')\n",
    "        pre_data_frame , count = individual_preprocess( word ,\n",
    "                                              [\"\\n\"] ,\n",
    "                                              count ,\n",
    "                                              [ \"\" , '' , '\\0'] ,\n",
    "                                              use_dictionary )\n",
    "        data_frame  = data_frame.append( pd.DataFrame( pre_data_frame ), ignore_index=True )\n",
    "    else:\n",
    "        for file in list_file :\n",
    "            word = read_all_line( directory , file ).split('|')\n",
    "            pre_data_frame , count = individual_preprocess( word ,\n",
    "                                              [\"\\n\"] ,\n",
    "                                              count ,\n",
    "                                              [ \"\" , '' , '\\0']  ,\n",
    "                                              use_dictionary )\n",
    "            data_frame  = data_frame.append( pd.DataFrame( pre_data_frame ), ignore_index=True )\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#directory of corpus\n",
    "corpus_directory = \"test_in_class\"\n",
    "corpus_files = (\"Politic9.txt\" , \"Allcolumn.txt\" , \"98JUL5_1.txt\" ,\n",
    "                \"98JUL5_2.txt\" , \"bkknews1.txt\" , \"Result1.txt\" ,\n",
    "                \"Result2(corpus-1).txt\" , \"Result3(corpus-2).txt\" ,\n",
    "                \"result4.txt\" , \"Result_c_2_1.txt\", \"Result_c_2_2.txt\" )\n",
    "corpus_test = ( \"POL1108.CUT\" , \"POL1805.CUT\" , \"POL1812.CUT\" ,  \n",
    "                \"POL1819.CUT\" , \"POL2008.CUT\" , \"POL2017.CUT\" ,\n",
    "                \"POL1109.CUT\" , \"POL1806.CUT\" , \"POL1813.CUT\" , \n",
    "                \"POL2001.CUT\" , \"POL2009.CUT\" , \"POL2018.CUT\" ,\n",
    "                \"POL1111.CUT\" , \"POL1807.CUT\" , \"POL1814.CUT\" , \n",
    "                \"POL2002.CUT\" , \"POL2010.CUT\" , \"POL2019.CUT\" ,\n",
    "                \"POL1112.CUT\" , \"POL1808.CUT\" , \"POL1815.CUT\" ,\n",
    "                \"POL2004.CUT\" , \"POL2011.CUT\" , \"POL2020.CUT\" ,\n",
    "                \"POL1113.CUT\" , \"POL1809.CUT\" , \"POL1816.CUT\" ,\n",
    "                \"POL2005.CUT\" , \"POL2012.CUT\" , \"POL1803.CUT\" ,\n",
    "                \"POL1810.CUT\" , \"POL1817.CUT\" , \"POL2006.CUT\" ,\n",
    "                \"POL2014.CUT\" , \"POL1804.CUT\" , \"POL1811.CUT\" ,\n",
    "                \"POL1818.CUT\" , \"POL2007.CUT\" , \"POL2015.CUT\" )\n",
    "all_corpus = corpus_files + corpus_test\n",
    "individual = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = preprocess( corpus_directory + \"/train_data\" , \n",
    "                        corpus_files, 1 , False , False )\n",
    "data_frame\n",
    "number_words = len( set( list( data_frame[\"Word\"].values ) ) )\n",
    "number_tags = len( list( set( data_frame[\"Tag\"].values ) ) )\n",
    "print( \"Have type of tag is \" + str( number_tags ) + \n",
    "        \" tags from \" + str(number_words) + \" words\")\n",
    "# print(set( list( data_frame[\"Tag\"].values )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow https://towardsdatascience.com/named-entity-recognition-and-classification-with-scikit-learn-f05372f07ba2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame['Sentence'].nunique(), data_frame.Word.nunique(), data_frame.Tag.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = data_frame.fillna(method='ffill')\n",
    "print( \"Number of sentence is {:d}\".format(data_frame['Sentence'].nunique()) )\n",
    "print( \"Number of word is {:d}\".format( data_frame.Word.nunique() ) )\n",
    "print( \"Number of tag is {:d}\".format( data_frame.Tag.nunique() ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame.groupby('Tag').size().reset_index(name='counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = data_frame.drop('Tag', axis=1)\n",
    "#vector_handle = DictVectorizer(sparse=False)\n",
    "#data_train = vector_handle.fit_transform( data_train.to_dict('records') )\n",
    "#value_train = data_frame.Tag.values"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# We want to remove label O because is common label don't have benefit to calculate about that"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now we have linear model Perceptron next we have to prepare data for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_frame = preprocess( corpus_directory + \"/test_tag\" , corpus_test , 1 , individual )\n",
    "#data_test = test_frame.drop('Tag', axis=1)\n",
    "#vector_handle = DictVectorizer(sparse=False)\n",
    "#data_test = vector_handle.fit_transform( data_test.to_dict('records') )\n",
    "#value_test = test_frame.Tag.values"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Next I will test model but we have problem about shape of test and train we must to have same shape of matrix to do machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame.shape[0] , test_frame.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_frame = data_frame.append( test_frame )\n",
    "all_data = all_frame.drop('Tag', axis=1)\n",
    "vector_handle = DictVectorizer(sparse=False)\n",
    "all_data = vector_handle.fit_transform(all_data.to_dict('records'))\n",
    "all_value = all_frame.Tag.values\n",
    "size_of_test = test_frame.shape[0]\n",
    "print( \"Propotional of test is \" + str( size_of_test ) )\n",
    "\n",
    "classes = np.unique( all_value )\n",
    "classes = classes.tolist()\n",
    "new_classes = classes.copy()\n",
    "new_classes.remove('O')\n",
    "new_classes.remove('')\n",
    "new_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(all_data, \n",
    "        all_value, \n",
    "        test_size = size_of_test, \n",
    "        shuffle = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Perceptron # the best from three model\n",
    "per = Perceptron(verbose=10, n_jobs=-1, max_iter=5)\n",
    "per.partial_fit(X_train, y_train, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_pred=suffix_processing( per.predict(X_test) ), \n",
    "                            y_true=y_test, labels=new_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Passive Aggressive Classifier # Not work\n",
    "# pa =PassiveAggressiveClassifier()\n",
    "# pa.partial_fit(X_train, y_train, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(classification_report(y_pred=pa.predict(X_test),\n",
    "                            #y_true=y_test, labels=new_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use naive bayes # NOT WORK\n",
    "# nb = MultinomialNB(alpha=0.01)\n",
    "# nb.partial_fit(X_train, y_train, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(classification_report(y_pred=nb.predict(X_test), \n",
    "                            #y_true=y_test, labels = new_classes))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
